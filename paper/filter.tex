\section{Particle Filter}
\label{sec:filter}
The algorithm implemented is based on the single-particle particle filter
described by Sanborn et al.\cite{sanborn2010} Justification for the use of a
particle filter to perform probabilistic inference for sequential clustering can
be found in their paper, in addition to a development of the models assumed for
this filter. Presented below is only a brief description of the underlying
probability distributions used, in addition to the modifications required to
tailor the particle filter to this experiment.

The posterior probability that a stimulus is assigned to a group is proportional
to a prior probability multiplied by a likelihood, as is always the case with
Bayesian inference. The prior probability encodes a preference over group sizes:
relatively how large groups should be, when new groups should be created, etc.
This prior must protect against overfitting, which in the context of category
learning would be creating a new categories for stimuli that should have been
grouped together. The likelihood function encodes the probability that the stimulus is drawn from
the same cluster that produced the stimuli already in the cluster. Using
primarily the same notation as Sanborn et al., this proportionality is represented by
\begin{equation}
P(z_i = k | \vec{X}_i = \vec{x}_i,  \vec{z}_{i-1}) \propto  P(\vec{X}_i =
\vec{x}_i | z_i = k,  \vec{z}_{i-1}) P(z_i = k | \vec{z}_{i-1})
\label{eq:posterior}
\end{equation}
$z_i = k$ notates assigning the $i^{th}$ stimulus to group $k$. $\vec{z}_{i-1}$ refers to
the categorization of the previous $i - 1$ stimuli. $\vec{X}_i$ is a random
variable for the $i^{th}$ stimulus, and $\vec{X}_i = \vec{x}_i$ notates the
random variable $\vec{X}_i$ taking the value $\vec{x}_i$. 
In Equation \ref{eq:posterior}, $P(\vec{X}_i =
\vec{x}_i | z_i = k,  \vec{z}_{i-1})$ represents the likelihood and $P(z_i = k |
\vec{z}_{i-1})$ represents the prior.

A Dirichlet process models the prior distribution over the probability that
any input stimuli will be grouped with a given cluster, whether that is one
of the existing clusters or would be a new cluster. This results in the
following form for the prior probability:
\begin{equation}
P(z_i = k | \vec{z}_{i-1}) = 
\begin{cases}
  \frac{M_k}{i - 1 + \alpha} & M_k > 0 \\
  \frac{\alpha}{i - 1 + \alpha} & M_k = 0 \\
\end{cases}
\end{equation}
$M_k$ is the number of stimuli in group $k$ after the previous $i - 1$ stimuli
had been sorted. This says the probability that the $i^{th}$ stimulus is placed
in group $k$ is proportional to the number of stimuli already in group $k$, or
to a parameter of the Dirichlet process, $\alpha$, if group $k$ would be a new
group. $\alpha$ is the dispersion parameter of the Dirichlet process; the larger
$\alpha$, the larger the probability that a stimulus will be assigned to a new
group. The value used for this parameter, as well as the values used for other
parameters of the particle filter, can be found in Table \ref{tab:parameters}.

The likelihood model for a stimulus being in a given group assumes that each
feature in a group follows a Gaussian distribution. The prior on the variance of
this Gaussian is modeled as an inverse $\chi^2$ distribution, and the prior on
the mean is modeled as another Gaussian. These priors result in the
likelihood function over each feature having the form of a Student's $t$
distribution with $a_i$ degrees of freedom. 

\begin{equation}
X_{i,d} | z_i = k,  \vec{z}_{i-1} \sim
t_{a_i}\left(\mu_i, \sigma_i^2 \left(1 + \frac{1}{\lambda_i}\right) \right)
\label{eq:feature-likelihood}
\end{equation}

where

\begin{align}
  \lambda_i &= \lambda_0 + M_k \\
  a_i &= a_0 + M_k \\
  \mu_i &= \frac{\lambda_0 \mu_0 + M_k \bar{x}}{\lambda_0 + M_k} \\
  \sigma_i^2 &= \frac{a_0 \sigma_0^2 + (n - 1) s^2 + \frac{\lambda_0
      M_k}{\lambda_0 + M_k} (\mu_0 - \bar{x})^2}{a_0 + M_k}\\
\end{align}

$X_{i,d}$ is a random variable for feature $d$ of the $i^{th}$ stimulus. In
Equation \ref{eq:feature-likelihood}, $X_{i,d}$ is conditioned on the $i^{th}$
being assigned to group $k$ and the group assignments of the previous $i-1$
stimuli. $M_k$ is again the number of elements in group $k$, but in this
instance assuming that the $i^{th}$ stimulus has been added to group $k$. The
prior mean is $\mu_0$, and the prior variance is $\sigma_0^2$, and the
confidences in the prior mean and prior variance are $\lambda_0$ and $a_0$,
respectively. $\mu_0$ is set to the midpoint of the potential values for each feature,
and $\sigma_0$ is set to be $1/8^{th}$ the range of the potential values.

The input are 100 by 100 pixel images, with each pixel taking on a grayscale
value between 0 and 255. Because the range for each feature is limited and
discrete, the Student's $t$ distribution was discretized and renormalized along
the valid range of the feature each time a feature likelihood value was
calculated.

In order to match the methodology used by Sanborn et al., the features are
treated as being independently distributed, so the likelihood for the entire
stimulus is simply a product over all the feature likelihoods:

\begin{equation}
P(\vec{X}_i = \vec{x}_i | z_i = k,  \vec{z}_{i-1}) =  \prod_d P(X_{i,d} =
x_{i,d} | z_i = k,  \vec{z}_{i-1})
\end{equation}

Additionally, the particle filter was only allowed to create 8 groups, to match
this limitation that was placed on human subjects.  The restriction was enacted
by not allowing the particle filter to consider placing a stimulus into a new
group after 8 groups had already been created.

\begin{table}
\centering
\begin{tabular}{c | c}
Parameter & Value \\ \hline
$\alpha$ & $30$ \\
$\mu_0$ & $127.5$ \\
$\lambda_0$ & $0.5$ \\
$\sigma_0$ & $32$ \\
$a_0$ & 2.0 \\
\end{tabular}
\caption{The parameters used for the particle filter. $\alpha$ is the dispersion
parameter for the Dirichlet process prior. $\mu_0$ and $\sigma_0$ are the mean
and standard deviation of the prior Gaussian distribution over each feature, and
$\lambda_0$ and $a_0$ are the confidence in the prior mean and the confidence in the
prior variance, respectively.}
\label{tab:parameters}
\end{table}
